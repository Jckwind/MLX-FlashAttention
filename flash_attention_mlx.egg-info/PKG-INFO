Metadata-Version: 2.1
Name: flash_attention_mlx
Version: 0.1.0
Summary: Flash Attention implementation using the MLX framework
Author-email: Jckwind <jckwind11@gmail.com>
License: MIT
Project-URL: Homepage, https://github.com/Jckwind/MLX-FlashAttention
Classifier: Programming Language :: Python :: 3
Classifier: License :: OSI Approved :: MIT License
Classifier: Operating System :: OS Independent
Requires-Python: >=3.11
Description-Content-Type: text/markdown
Requires-Dist: mlx
Requires-Dist: torch
Requires-Dist: numpy

# Flash Attention Implementation in MLX

This project demonstrates the implementation of Flash Attention using the MLX framework. Flash Attention is an efficient attention mechanism designed to reduce memory usage and computational overhead, making it suitable for large-scale models and long sequences.
